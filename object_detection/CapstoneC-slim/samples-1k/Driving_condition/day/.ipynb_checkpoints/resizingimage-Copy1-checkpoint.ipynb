{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resizeimage import resizeimage\n",
    "#!pip install python-resize-image\n",
    "import pandas as pd\n",
    "import os, sys, glob\n",
    "import math\n",
    "import warnings\n",
    "import PIL\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, imageName, basewidth=600, Rootpath='/home/kevimwe/tensorflow/models/research/object_detection/CapstoneB/samples-1k/Driving_condition/day/'):\n",
    "    path = Rootpath+\"images_Resized/\"\n",
    "    import os, errno\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    img = Image.open(image)\n",
    "    wpercent = (basewidth / float(img.size[0]))\n",
    "    hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "    img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)\n",
    "    img.save(path+imageName+ \"_reSized.jpg\")\n",
    "    return(img.size[0],img.size[1] , imageName+ \"_reSized.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "def imageDATA(Rootpath, size=600):\n",
    "    images_data = pd.DataFrame()\n",
    "    image_Name =[]\n",
    "    image_Original_Width =[]\n",
    "    image_Original_Height =[]\n",
    "    image_Scaled_Width =[]\n",
    "    image_Scaled_Height =[] \n",
    "    new_filename =[] \n",
    "    #path = os.getcwd()+\"/image_resizing_day/\"\n",
    "    path_wd = Rootpath+'images/'\n",
    "    os.chdir(path_wd)\n",
    "    images = glob.glob(\"*.jpg\")\n",
    "    for image in images:\n",
    "        imageName = image.split('.')[0]\n",
    "        X = Image.open(image).size[0]\n",
    "        Y = Image.open(image).size[0]\n",
    "        X_new, Y_new, Z_new = resize(image, imageName, size)\n",
    "        image_Name =image_Name + [image]\n",
    "        image_Original_Width =image_Original_Width+[Y]\n",
    "        image_Original_Height = image_Original_Height+[Y]\n",
    "        image_Scaled_Width =image_Scaled_Width+ [X_new]\n",
    "        image_Scaled_Height =image_Scaled_Height+ [Y_new]\n",
    "        new_filename =new_filename+ [Z_new]\n",
    "\n",
    "    images_data = pd.DataFrame({'filename':image_Name ,'new_filename':new_filename  ,'width':image_Original_Width ,'height':image_Original_Height,\n",
    "                                'width_scaled':image_Scaled_Width , 'height_scaled':image_Scaled_Height })  \n",
    "    return(images_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GroundTruthScaler(df2, Original_GT):\n",
    "    df1 = Original_GT[Original_GT.filename.isin(list(df2.filename))] #limiting data to just the new images\n",
    "    image_Scaled_Width =[]\n",
    "    image_Scaled_Height =[] \n",
    "    newFilename =[]\n",
    "    #Merge the two datasets\n",
    "    \n",
    "    f = lambda x:int(x) #into expect some rounding error\n",
    "    \n",
    "    for index, row in df1.iterrows():\n",
    "        #print(row.filename)\n",
    "        image_Scaled_Width = image_Scaled_Width + [int(df2[df2.filename==row.filename].width_scaled)]\n",
    "        image_Scaled_Height = image_Scaled_Height + [int(df2[df2.filename==row.filename].height_scaled)]\n",
    "        newFilename = newFilename + [str(row.filename).split('.')[0]+'_reSized.jpg']\n",
    "        #print(str(row.filename).split('.'))\n",
    "    df1['New_filename'] =  newFilename\n",
    "    df1['widthScaled'] = image_Scaled_Width\n",
    "    df1['heightScaled'] = image_Scaled_Height\n",
    "    df1['Width%Scaler'] = 1+(df1.widthScaled -df1.width )/df1.width   \n",
    "    df1['Height%Scaler'] = 1+(df1.heightScaled -df1.height )/df1.height\n",
    "    df1['xmin_new'] = [f(x) for x in df1['xmin']*df1['Width%Scaler']]\n",
    "    df1['ymin_new'] =[f(x) for x in df1['ymin']*df1['Height%Scaler']]\n",
    "    df1['xmax_new'] =[f(x) for x in df1['xmax']*df1['Width%Scaler']]\n",
    "    df1['ymax_new'] =[f(x) for x in df1['ymax']*df1['Height%Scaler']]\n",
    "    list_ =['filename','New_filename', 'width', 'height', 'class','widthScaled', 'heightScaled','xmin', 'ymin', 'xmax', 'ymax','id', 'xmin_scaled', 'ymin_scaled', 'xmax_scaled', 'ymax_scaled', 'xmin_new', 'ymin_new','xmax_new',  'ymax_new']\n",
    "    FinalDataGTScaled = df1[list_]\n",
    "    FinalDataGTScaled['xmin_new_scaled'] = FinalDataGTScaled['xmin_new']/ FinalDataGTScaled['widthScaled']\n",
    "    FinalDataGTScaled['ymin_new_scaled'] =FinalDataGTScaled['ymin_new']/ FinalDataGTScaled['heightScaled'] \n",
    "    FinalDataGTScaled['xmax_new_scaled'] =FinalDataGTScaled['xmax_new']/ FinalDataGTScaled['widthScaled'] \n",
    "    FinalDataGTScaled['ymax_new_scaled'] =FinalDataGTScaled['ymax_new']/ FinalDataGTScaled['heightScaled'] \n",
    "    return(FinalDataGTScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/datitran/raccoon_dataset/blob/master/draw%20boxes.ipynb\n",
    "def draw_boxes(image_name, imagedir, GroundTruthLabels, T =1):\n",
    "    \n",
    "    '''\n",
    "    Function draws sample bounding boxes to verify scaling on groundtruth cordinates to images\n",
    "    '''\n",
    "    \n",
    "    if T == 1: #original Image\n",
    "        GroundTruth_value = GroundTruthLabels[GroundTruthLabels.filename == image_name]\n",
    "        img = cv2.imread(imagedir+image_name)\n",
    "        for index, row in GroundTruth_value.iterrows():\n",
    "            img = cv2.rectangle(img, (row.xmin,row.ymin),(row.xmax,row.ymax),(0, 255, 0), 3)\n",
    "    else:\n",
    "        GroundTruth_value = GroundTruthLabels[GroundTruthLabels.New_filename == image_name]\n",
    "        img = cv2.imread(imagedir+image_name)\n",
    "        for index, row in GroundTruth_value.iterrows():\n",
    "            img = cv2.rectangle(img, (row.xmin_new,row.ymin_new),(row.xmax_new,row.ymax_new),(0, 255, 0), 1)\n",
    "        \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from random import randint\n",
    "import math \n",
    "import random\n",
    "\n",
    "def chunks(lst, n=2):\n",
    "    '''\n",
    "    Function to split image list into train, test and val\n",
    "    '''\n",
    "    Y = np.array_split(lst,n) #split data into two one part for training\n",
    "    Train = Y[0]\n",
    "    X = np.array_split(Y[1],n)\n",
    "    Test = X[0]\n",
    "    Validation = X[1]\n",
    "    return(Train, Test, Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def  Data_Split(newGTfile, path):\n",
    "    \n",
    "    '''\n",
    "    Function to to split scaled data into test train and validation \n",
    "    and save them into there repective directory for transfer learning\n",
    "    \n",
    "    Train:  object_detection/images/train [images, train_labels.csv] \n",
    "    Test:   object_detection/images/test [images, test_labels.csv] \n",
    "    Validation: object_detection/samples-1k/[day, night, rainy, snowy]/images + ground_truth.csv\n",
    "    \n",
    "    '''\n",
    "\n",
    "    image_list= pd.unique(newGTfile.New_filename) #images list\n",
    "    \n",
    "    Train_list, Test_list, Val_list = chunks(image_list) #split images list\n",
    "    \n",
    "    filter_column = ['New_filename', 'widthScaled','heightScaled', 'class', 'xmin_new',\n",
    "                     'ymin_new','xmax_new', 'ymax_new', 'xmin_new_scaled', 'ymin_new_scaled',\n",
    "                     'xmax_new_scaled', 'ymax_new_scaled','id']\n",
    "    #filename\twidth\theight\tclass\txmin\tymin\txmax\tymax\n",
    "    final_name =['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax', 'xmin_scaled',\n",
    "       'ymin_scaled', 'xmax_scaled', 'ymax_scaled', 'id']\n",
    "    \n",
    "    FinalDataGT = newGTfile[filter_column] #filter columns\n",
    "    display(FinalDataGT.head())\n",
    "    FinalDataGT.columns = final_name # rename columns\n",
    "    # Split Groundtruth files for transfer learning\n",
    "    Train_DF = FinalDataGT[FinalDataGT.filename.isin(list(Train_list))]\n",
    "    Test_DF = FinalDataGT[FinalDataGT.filename.isin(list(Test_list))]\n",
    "    Val_DF = FinalDataGT[FinalDataGT.filename.isin(list(Val_list))]\n",
    "    \n",
    "    \n",
    "    # Split desired images into folders copy to over from images_resised folder\n",
    "    # Check if val, test and train \n",
    "    \n",
    "    import os, errno, shutil\n",
    "    try:\n",
    "        #validation\n",
    "        os.makedirs(path+'images/images/Val/')\n",
    "        Val_DF.to_csv(path+'images/images/val_labels.csv', index=False)  \n",
    "        # Test folder\n",
    "        os.makedirs(path+'images/images/Test/')\n",
    "        Test_DF.to_csv(path+'images/images/test_labels.csv', index=False)\n",
    "        os.makedirs(path+'images/images/Train/')\n",
    "        Train_DF.to_csv(path+'images/images/train_labels.csv', index=False)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    \n",
    "    #Copy images files over fot desired folders from Resized folder\n",
    "    os.chdir(path+'/images_Resized/') #Change working directory\n",
    "    for f in Val_list:\n",
    "        shutil.copy(f, path+'images/images/Val/')\n",
    "    for f in Test_list:\n",
    "        shutil.copy(f, path+'images/images/Test/')\n",
    "    for f in Train_list:\n",
    "        shutil.copy(f, path+'images/images/Train/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#       Main          #\n",
    "#######################\n",
    "\n",
    "def Run_ImageScaler(Rootpath ='/home/kevimwe/tensorflow/models/research/object_detection/CapstoneB/samples-1k/Driving_condition/',size=600, condition ='day'):\n",
    "    images_data = imageDATA(Rootpath+condition+'/', size)\n",
    "    #imageDATA('/home/kevimwe/tensorflow/models/research/object_detection/CapstoneB/samples-1k/Driving_condition/day/')\n",
    "\n",
    "    \n",
    "    os.chdir(Rootpath+condition+'/')\n",
    "    day_GT = pd.read_csv(condition+\"_groundTruthlabels.csv\")\n",
    "    #display(day_GT)\n",
    "\n",
    "    #scaled data\n",
    "    Temp2 =GroundTruthScaler(images_data, day_GT)\n",
    "    \n",
    "\n",
    "    \n",
    "    GroundTruth =Temp2##pd.read_csv(File)\n",
    "    imageslist  =pd.unique(GroundTruth.filename)\n",
    "    #print(imageslist)\n",
    "    path =Rootpath+condition+'/images/'\n",
    "    path2 =Rootpath+condition+'/images_Resized/'\n",
    "\n",
    "    i=random.randint(0, len(imageslist))\n",
    "\n",
    "    #sample scaling\n",
    "    display(Image.fromarray(draw_boxes(imageslist[i], path, GroundTruth, 1)))\n",
    "    display(Image.fromarray(draw_boxes(imageslist[i].split('.')[0]+'_reSized.jpg', path2, GroundTruth, 0)))\n",
    "\n",
    "\n",
    "    Temp2.to_csv(condition+\"_Width_\"+size+\"_ScaledgroundTruthlabels.csv\", index=False)\n",
    "\n",
    "\n",
    "    Data_Split(pd.read_csv(condition+'_ScaledgroundTruthlabels.csv'),  Rootpath+condition+'/')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Success!!!\")\n",
    "    \n",
    "    return(Temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/kevimwe/tensorflow/models/research/object_detection/CapstoneB/samples-1k/Driving_condition/day/images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-faa550428776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTempX\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mRun_ImageScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b2e6228d85ed>\u001b[0m in \u001b[0;36mRun_ImageScaler\u001b[0;34m(Rootpath, size, condition)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRun_ImageScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRootpath\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'/home/kevimwe/tensorflow/models/research/object_detection/CapstoneB/samples-1k/Driving_condition/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimages_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageDATA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRootpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#imageDATA('/home/kevimwe/tensorflow/models/research/object_detection/CapstoneB/samples-1k/Driving_condition/day/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-fa649a3d4700>\u001b[0m in \u001b[0;36mimageDATA\u001b[0;34m(Rootpath, size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#path = os.getcwd()+\"/image_resizing_day/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpath_wd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRootpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'images/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_wd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/kevimwe/tensorflow/models/research/object_detection/CapstoneB/samples-1k/Driving_condition/day/images/'"
     ]
    }
   ],
   "source": [
    "\n",
    "TempX =Run_ImageScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
